from bs4 import BeautifulSoup
import os
import requests
import shutil


'''Data module is using to write all the IR page content and return all the links to the .pdf file of 
the annual report.'''


def write_html_content(website_url):
    """This function write all the site content by a given URL.
    By the end, will have
    1. txt file contain all site HTML.
    2. txt file with all the link of the site
    3. an empty txt file for the filtered link.
    4. those files will be in a folder we generated by the site name.
    """
    if website_url.count('.') >= 2:
        i1 = website_url.index(".")
        i2 = website_url.index(".", i1 + 1)
        site = website_url[i1 + 1:i2]
    else:
        i1 = website_url.index("://")
        i2 = website_url.index(".", i1 + 1)
        site = website_url[i1 + 3:i2]
    print("the site is:", site)

    x = site.count('/')
    if x > 0:
        site = "My_site"

    # create the "docs" directory if not exist
    docs_dir = os.path.join(os.getcwd(), "docs")
    os.makedirs(docs_dir, exist_ok=True)

    # create a folder for each new site 
    site_folder = os.path.join(docs_dir, site)
    os.makedirs(site_folder, exist_ok=True)

    html_file_path = os.path.join(site_folder, site + "_HtmlToText.txt")

    # Writing all the site content and the link, from the given URL
    try:
        response = requests.get(website_url)
        html_content = response.text
        print(len(html_content))  # Print the length of the HTML content

        with open(html_file_path, "w", encoding="utf-8") as file:
            file.write(html_content)
        print(f"HTML content saved to {html_file_path}")

        return html_file_path
    except requests.exceptions.RequestException as e:
        print(f"Error: {e}")


def read_html(website_url, html_file_path, filter_keywords):
    """After using "write_page_html" we have all the site content.
      this function will fillter
    all the link by the given "fillter_keywords" and write all the filtered link in a new file."""
    with open(html_file_path, 'r', encoding='utf-8') as html_file_path:
        html_content = html_file_path.read()

    soup = BeautifulSoup(html_content, 'html.parser')

    links = soup.find_all('a')
    pdf_repo = []

    dir_name = str(os.path.dirname(html_file_path.name))
    all_link_file = os.path.join(dir_name, "all_links.txt")
    filtered_link_file = os.path.join(dir_name, "filtered_links.txt")

    with open(all_link_file, 'w') as all_link_file:
        # get all the pdf links 
        for link in links:
            try:
                url = link.get('href')
                if url is not None:
                    if '/' in url and url.index('/') == 0:
                        url = os.path.join(website_url, url[1:])
                    all_link_file.write(url + '\n')  # Write the 'href' attribute as a string
                    # add to pdf_repo based on the "filtered keywords"
                    if any(keyword in url for keyword in filter_keywords):
                        pdf_repo.append(url)
            except Exception as e:
                print(url)
                print(f"Error processing URL: {e}")
                # You can choose to continue processing other URLs or break the loop here

    # Write the filtered links to the link file
    with open(filtered_link_file, 'w') as filtered_link_file:
        for l in pdf_repo:
            filtered_link_file.write(l + '\n')

    return filtered_link_file


def download_pdf(filtered_link_file):
    """After have all the fillterd link as we want in a file..
    This function download all of those link to a folder called "report" as a .pdf file."""
    report_folder = os.path.join(os.path.dirname(filtered_link_file), "report")
    if not os.path.exists(report_folder):
        os.mkdir(report_folder)

    pdf_urls = []
    with open(filtered_link_file, 'r') as file:
        for link in file:
            pdf_urls.append(link.strip())

    download_files = []
    for pdf_u in pdf_urls:
        # Extract the filename from the URL
        filename = os.path.join(report_folder, pdf_u.split("/")[-1])

        # Send a GET request to download the PDF
        response = requests.get(pdf_u)

        if response.status_code == 200:
            # Save the PDF file
            with open(filename, 'wb') as pdf_file:
                pdf_file.write(response.content)
            print(f"Downloaded: {filename}")
            download_files.append(filename)
        else:
            print(f"Failed to download: {pdf_u}")

    # if the file doesn't have an extension of pdf -> add it.
    files = os.listdir(report_folder)

    for file_name in files:
        file_path = os.path.join(report_folder, file_name)

        # Check if the file doesn't have an extension
        if "." not in file_name:
            # Rename the file by adding ".pdf" as the extension
            new_file_name = file_name + ".pdf"
            new_file_path = os.path.join(report_folder, new_file_name)

            # Rename the file using shutil.move()
            shutil.move(file_path, new_file_path)

            print(f"Renamed: {file_name} -> {new_file_name}")
